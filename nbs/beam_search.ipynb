{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import pdb\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "from utils import utils\n",
    "from utils.metrics import BleuScore\n",
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '/project/cq-training-1/project2/teams/team12/data/'\n",
    "best_model_path = '/project/cq-training-1/project2/submissions/team12/low-resource-translation/saved_model/Transformer-num_layers_2-d_model_128-num_heads_8-dff_512_fr_to_en_False_embedding_None_embedding_dim_128_back_translation_True_ratio_4.0'\n",
    "path_en = os.path.join(data_dir, 'train.lang1')\n",
    "path_fr = os.path.join(data_dir, 'train.lang2')\n",
    "\n",
    "\n",
    "# Create vocabs\n",
    "word2idx_en, idx2word_en = utils.create_vocab(path_en, vocab_size=None)\n",
    "word2idx_fr, idx2word_fr = utils.create_vocab(path_fr, vocab_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = utils.load_training_data(path_en, path_fr, word2idx_en, word2idx_fr, seq_len=150, batch_size=64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional methods \n",
    "def get_next(self,x,y):\n",
    "    preds,_ = self.forward(x, y, training=False)\n",
    "    return preds[:,-1,:], preds\n",
    "\n",
    "def update_state(self, y_true, y_pred, vocab, idx=False):\n",
    "    for i in range(len(y_true)):\n",
    "        label_sentence = utils.generate_sentence(y_true[i].numpy().astype('int'), vocab)\n",
    "        if idx: pred_sentence = utils.generate_sentence(y_pred[i], vocab) \n",
    "        else:   pred_sentence = utils.generate_sentence_from_probabilities(y_pred[i].numpy(), vocab)\n",
    "        self.total_score += sacrebleu.sentence_bleu(pred_sentence, label_sentence, smooth_method='exp').score\n",
    "        self.total_num_examples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer.get_next = get_next\n",
    "BleuScore.update_state = update_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2b8a35799c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_config = {'num_layers': 2, 'd_model': 128, 'dff': 512, 'num_heads': 8}\n",
    "model = Transformer(model_config, len(word2idx_en), word2idx_fr)\n",
    "model.load_weights(os.path.join(best_model_path, \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_beam_search(model,batch,steps,width,word2idx_fr):\n",
    "    \n",
    "    x,bs = batch['inputs'], batch['inputs'].shape[0] \n",
    "    # adjust for start\n",
    "    hist = [np.ones((1,bs,1),dtype=np.int32)* model.start_token] \n",
    "    for i in range(1,steps): \n",
    "        # length of input = i+1 at each timestep\n",
    "        hist.append(np.ones((width,bs,i+1),dtype=np.int32)) \n",
    "    hist_probs = np.zeros((steps,width,bs),dtype=np.float32)\n",
    "    for i in range(1,steps): # loop over steps\n",
    "        wid = hist[i-1].shape[0] # adjust for start\n",
    "        # total candidates = width*width at each timestep\n",
    "        cand, cand_probs = np.zeros((bs,wid*width,i+1),dtype=np.int32), np.zeros((bs,wid*width),dtype=np.float32) \n",
    "        for j in range(wid): # loop over width elements\n",
    "            \n",
    "            # output of prev step is current input step \n",
    "            curr, curr_probs = tf.convert_to_tensor(hist[i-1][j]), tf.convert_to_tensor(hist_probs[i-1,j])\n",
    "            \n",
    "            temp_idx = np.flatnonzero(curr[:,-1] == word2idx_fr['<end>']) # check for end \n",
    "            if temp_idx.size > 0: # eager tensor does not support item assigment\n",
    "                temp_var = curr_probs.numpy()\n",
    "                temp_var[temp_idx] += - 100\n",
    "                curr_probs = tf.convert_to_tensor(temp_var)\n",
    "                \n",
    "            preds,_ = model.get_next(x,curr)\n",
    "            preds = tf.nn.softmax(preds,-1)\n",
    "\n",
    "            top_sort= tf.argsort(preds,axis=-1,direction='DESCENDING')    \n",
    "            topk = top_sort[:,:width] # take top 'width' predictions\n",
    "            unk_idx = tf.where(topk == word2idx_fr['<unk>']).numpy()\n",
    "            # replace <unk> with next best \n",
    "            if unk_idx.size > 0: \n",
    "                temp = topk.numpy() # eager tensor does not support item assigment\n",
    "                temp[unk_idx[:,0],unk_idx[:,1]] = top_sort.numpy()[unk_idx[:,0],width+1]\n",
    "                topk = tf.convert_to_tensor(temp)\n",
    "                                \n",
    "            topk_probs = tf.gather(preds,topk,axis=-1,batch_dims=-1) # take top 'width' probs\n",
    "            curr = tf.broadcast_to(tf.expand_dims(curr,1),(bs,width,curr.shape[-1])) # bs, width, i\n",
    "            topk = tf.expand_dims(topk,-1) # shape = bs, width,1\n",
    "            cand[:,j*width:(j+1)*width] = tf.concat([curr,topk],-1) # next step shape = current_shape + 1 \n",
    "            cand_probs[:,j*width:(j+1)*width] = curr_probs[:,None] + np.log(topk_probs) # add log probs\n",
    "        \n",
    "        cand, cand_probs = tf.convert_to_tensor(cand), tf.convert_to_tensor(cand_probs)\n",
    "        indices = tf.argsort(cand_probs,axis=-1,direction='DESCENDING')[:,:width] # from candidates = width*width pick width\n",
    "        value = tf.gather(cand,indices,axis=1,batch_dims=1)\n",
    "        hist[i] = tf.transpose(value,perm=(1,0,2)).numpy() # store next step inputs\n",
    "        hist_probs[i] = tf.transpose(tf.gather(cand_probs,indices,axis=-1,batch_dims=1)).numpy() # store probs\n",
    "    \n",
    "    return hist, hist_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beam(model,batch,word2idx_fr,steps=134,width=5):\n",
    "    options, probs = tf_beam_search(model,batch,steps,width,word2idx_fr)\n",
    "    options,probs = options[1:], probs[1:]\n",
    "    return options,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(options,probs,alpha,skip):\n",
    "    # length normalization\n",
    "    probs = probs / (np.arange(1,134)**alpha).reshape(-1,1,1)\n",
    "    options,probs = options[skip:], probs[skip:]\n",
    "    preds = []\n",
    "    for i in range(probs.shape[2]):\n",
    "        idx = np.unravel_index(np.argmax(probs[:,:,i]),probs[:,:,i].shape)\n",
    "        preds.append(options[idx[0]][idx[1],i])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_end(ops,probs,word2idx_fr,bonus):\n",
    "    # bonus scores for <end>\n",
    "    for step in range(len(ops)):\n",
    "        idx = np.where(ops[step][...,-1] == word2idx_fr['<end>'])\n",
    "        if idx[0].size > 0:\n",
    "            probs[step][idx[0],idx[1]] = probs[step][idx[0],idx[1]] + bonus*step\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [32:35<00:00, 75.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Do a beam search run\n",
    "batch_ops, batch_probs =[], []\n",
    "for batch in tqdm(valid_dataset,total = 26):    \n",
    "    ops,probs = get_various_runs(model,batch,word2idx_fr,bonus=0)\n",
    "    batch_ops.append(ops)\n",
    "    batch_probs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_ops = copy.deepcopy(batch_ops)\n",
    "# copy_probs = copy.deepcopy(batch_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:00, 103.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all valid batches\n",
    "valid_batches = []\n",
    "for batch in tqdm(valid_dataset):\n",
    "    valid_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different hyper-parameters\n",
    "bonus = [0,0.9,1,1.1]\n",
    "alpha = [0.6,0.7,0.8,0.9,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust probs for different bonus settings\n",
    "probs = []\n",
    "for i,b in enumerate(bonus):\n",
    "    temp_probs = []\n",
    "    for j, b_ops in enumerate(batch_ops):\n",
    "        new_probs = copy.deepcopy(batch_probs[j])\n",
    "        new_probs = check_end(b_ops,new_probs,b)\n",
    "        temp_probs.append(new_probs)\n",
    "    probs.append(temp_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_beam = BleuScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score for bonus: 0 , Alpha: 0.6 = 5.240347498592297\n",
      "Bleu Score for bonus: 0 , Alpha: 0.7 = 5.538980504401233\n",
      "Bleu Score for bonus: 0 , Alpha: 0.8 = 5.912006534638225\n",
      "Bleu Score for bonus: 0 , Alpha: 0.9 = 6.299427800743217\n",
      "Bleu Score for bonus: 0 , Alpha: 1 = 6.799250492264565\n",
      "\n",
      "\n",
      "Bleu Score for bonus: 0.9 , Alpha: 0.6 = 13.788507398297382\n",
      "Bleu Score for bonus: 0.9 , Alpha: 0.7 = 13.76859594097073\n",
      "Bleu Score for bonus: 0.9 , Alpha: 0.8 = 13.802057637823957\n",
      "Bleu Score for bonus: 0.9 , Alpha: 0.9 = 13.760269066166185\n",
      "Bleu Score for bonus: 0.9 , Alpha: 1 = 13.71499271622439\n",
      "\n",
      "\n",
      "Bleu Score for bonus: 1 , Alpha: 0.6 = 13.772281234631146\n",
      "Bleu Score for bonus: 1 , Alpha: 0.7 = 13.83306519896875\n",
      "Bleu Score for bonus: 1 , Alpha: 0.8 = 13.78764660173033\n",
      "Bleu Score for bonus: 1 , Alpha: 0.9 = 13.762686680913168\n",
      "Bleu Score for bonus: 1 , Alpha: 1 = 13.712780026052592\n",
      "\n",
      "\n",
      "Bleu Score for bonus: 1.1 , Alpha: 0.6 = 13.70374266193462\n",
      "Bleu Score for bonus: 1.1 , Alpha: 0.7 = 13.784217844868536\n",
      "Bleu Score for bonus: 1.1 , Alpha: 0.8 = 13.795527346605217\n",
      "Bleu Score for bonus: 1.1 , Alpha: 0.9 = 13.799726191908935\n",
      "Bleu Score for bonus: 1.1 , Alpha: 1 = 13.708038940978636\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Bleu Score\n",
    "for i,b in enumerate(bonus):\n",
    "    for k,alp in enumerate(alpha):\n",
    "        bleu_beam.reset_states()\n",
    "        for j,b_o in enumerate(batch_ops):\n",
    "            b_p = copy.deepcopy(probs[i][j]) \n",
    "            preds = process_batch(b_o,b_p,alp,skip=5)\n",
    "            bleu_beam.update_state(valid_batches[j]['labels'], preds, idx2word_fr, idx = True)\n",
    "        print(f\"Bleu Score for bonus: {b} , alpha: {alp} = {bleu_beam.result()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [34:24<00:00, 79.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 13.83306519896875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "score = BleuScore()\n",
    "for batch in tqdm(valid_dataset,total=26):\n",
    "    options, probs = get_beam(model,batch,word2idx_fr)\n",
    "    probs = check_end(options,probs,word2idx_fr,bonus=1)\n",
    "    preds = process_batch(options,probs,alpha=0.7,skip=5)\n",
    "    score.update_state(batch['labels'], preds, idx2word_fr, idx = True)\n",
    "    \n",
    "print(f\"BLEU Score: {score.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final function, with default parameters\n",
    "def beam_search(model, batch, word2idx, bonus=1, alpha=0.7, skip=5):\n",
    "    options, probs = get_beam(model,batch, word2idx)\n",
    "    probs = check_end(options, probs, word2idx, bonus)\n",
    "    preds = process_batch(options, probs, alpha, skip)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some examples\n",
    "preds = beam_search(model,valid_batches[0],word2idx_fr)\n",
    "output = model(valid_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:\n",
      "as we know the reduction of regional disparities is one of the fundamental aims of the eu\n",
      "\n",
      "Target:\n",
      "Comme nous le savons , la disparition des disparités régionales constitue un des objectifs fondamentaux de l' ue .\n"
     ]
    }
   ],
   "source": [
    "idx=11\n",
    "print(\"Source:\")\n",
    "print(utils.generate_sentence(valid_batches[0]['inputs'][idx].numpy(),idx2word_en))\n",
    "print(\"\\nTarget:\")\n",
    "print(utils.generate_sentence(valid_batches[0]['labels'][idx].numpy(),idx2word_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Prediction:\n",
      "Comme nous le savons la réduction des émissions de conum , il est essentiel de réduire les émissions de conum .\n",
      "\n",
      "Beam Search Prediction:\n",
      "Comme nous le savons tous , la réduction des disparités régionales est un des objectifs fondamentaux de l' ue .\n"
     ]
    }
   ],
   "source": [
    "print(\"Greedy Prediction:\")\n",
    "print(utils.generate_sentence_from_probabilities(output[idx],idx2word_fr))\n",
    "\n",
    "print(\"\\nBeam Search Prediction:\")\n",
    "print(utils.generate_sentence(preds[idx],idx2word_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
