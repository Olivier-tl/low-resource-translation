{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import pdb\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import utils\n",
    "from utils.metrics import BleuScore\n",
    "from models.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_dir = '/project/cq-training-1/project2/teams/team12/data/'\n",
    "best_model_path = '/project/cq-training-1/project2/submissions/team12/low-resource-translation/saved_model/Transformer-num_layers_2-d_model_128-num_heads_8-dff_512_fr_to_en_False_embedding_None_embedding_dim_128_back_translation_True_ratio_4.0'\n",
    "path_en = os.path.join(data_dir, 'train.lang1')\n",
    "path_fr = os.path.join(data_dir, 'train.lang2')\n",
    "\n",
    "\n",
    "# Create vocabs\n",
    "word2idx_en, idx2word_en = utils.create_vocab(path_en, vocab_size=None)\n",
    "word2idx_fr, idx2word_fr = utils.create_vocab(path_fr, vocab_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = utils.load_training_data(path_en, path_fr, word2idx_en, word2idx_fr, seq_len=150, batch_size=64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional methods \n",
    "def get_next(self,x,y):\n",
    "    preds = self.forward(x, y, training=False)\n",
    "    return preds[:,-1,:], preds\n",
    "\n",
    "def update_state(self, y_true, y_pred, vocab, idx=False):\n",
    "    for i in range(len(y_true)):\n",
    "        label_sentence = utils.generate_sentence(y_true[i].numpy().astype('int'), vocab)\n",
    "        if idx: pred_sentence = utils.generate_sentence(y_pred[i], vocab) \n",
    "        else:   pred_sentence = utils.generate_sentence_from_probabilities(y_pred[i].numpy(), vocab)\n",
    "        self.total_score += sacrebleu.sentence_bleu(pred_sentence, label_sentence, smooth_method='exp').score\n",
    "        self.total_num_examples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer.get_next = get_next\n",
    "BleuScore.update_state = update_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2ad323081390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_config = {'num_layers': 2, 'd_model': 128, 'dff': 512, 'num_heads': 8}\n",
    "model = Transformer(model_config, len(word2idx_en), word2idx_fr)\n",
    "model.load_weights(os.path.join(best_model_path, \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_beam_search(model,batch,steps,width,word2idx_fr,bonus=0):\n",
    "    \n",
    "    x,bs = batch['inputs'], batch['inputs'].shape[0] \n",
    "    # adjust for start\n",
    "    hist = [np.ones((1,bs,1),dtype=np.int32)* model.start_token] \n",
    "    for i in range(1,steps): \n",
    "        # length of input = i+1 at each timestep\n",
    "        hist.append(np.ones((width,bs,i+1),dtype=np.int32)) \n",
    "    hist_probs = np.zeros((steps,width,bs),dtype=np.float32)\n",
    "    flag = False\n",
    "    for i in range(1,steps): # loop over steps\n",
    "        wid = hist[i-1].shape[0] # adjust for start\n",
    "        # total candidates = width*width at each timestep\n",
    "        cand, cand_probs = np.zeros((bs,wid*width,i+1),dtype=np.int32), np.zeros((bs,wid*width),dtype=np.float32) \n",
    "        for j in range(wid): # loop over width elements\n",
    "            \n",
    "            temp_idx = np.nonzero(hist[i-1][j] == word2idx_fr['<end>'])[0]\n",
    "            if temp_idx.size > 0:\n",
    "                hist_probs[i-1,j][temp_idx] += bonus*(i-1)\n",
    "            \n",
    "            # output of prev step is current input step \n",
    "            curr, curr_probs = tf.convert_to_tensor(hist[i-1][j]), tf.convert_to_tensor(hist_probs[i-1,j])\n",
    "            \n",
    "            temp_idx = np.flatnonzero(curr[:,-1] == word2idx_fr['<end>']) # check for end \n",
    "            if temp_idx.size > 0: # eager tensor does not support item assigment\n",
    "                temp_var = curr_probs.numpy()\n",
    "                temp_var[temp_idx] += - 100\n",
    "                curr_probs = tf.convert_to_tensor(temp_var)\n",
    "                \n",
    "            preds,_ = model.get_next(x,curr)\n",
    "            preds = tf.nn.softmax(preds,-1)\n",
    "            topk= tf.argsort(preds,axis=-1,direction='DESCENDING')[:,:width] # take top 'width' predictions\n",
    "            topk_probs = tf.sort(preds,axis=-1,direction='DESCENDING')[:,:width] # take top 'width' probs\n",
    "            curr = tf.broadcast_to(tf.expand_dims(curr,1),(bs,width,curr.shape[-1])) # bs, width, i\n",
    "            topk = tf.expand_dims(topk,-1) # shape = bs, width,1\n",
    "            cand[:,j*width:(j+1)*width] = tf.concat([curr,topk],-1) # next step shape = current_shape + 1 \n",
    "            cand_probs[:,j*width:(j+1)*width] = curr_probs[:,None] + np.log(topk_probs) # add log probs\n",
    "        \n",
    "        cand, cand_probs = tf.convert_to_tensor(cand), tf.convert_to_tensor(cand_probs)\n",
    "        indices = tf.argsort(cand_probs,axis=-1,direction='DESCENDING')[:,:width] # from candidates = width*width pick width\n",
    "        value = tf.gather(cand,indices,axis=1,batch_dims=1)\n",
    "        hist[i] = tf.transpose(value,perm=(1,0,2)).numpy() # store next step inputs\n",
    "        hist_probs[i] = tf.transpose(tf.gather(cand_probs,indices,axis=-1,batch_dims=1)).numpy() # store probs\n",
    "    \n",
    "    return hist, hist_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_various_runs(model,batch,word2idx_fr,steps=134,width=5,bonus=0):\n",
    "    options, probs = tf_beam_search(model,batch,steps,width,word2idx_fr,bonus)\n",
    "    options,probs = options[1:], probs[1:]\n",
    "    return options,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(options,probs,alpha,skip):\n",
    "    probs = probs / (np.arange(1,150)**alpha).reshape(-1,1,1)\n",
    "    options,probs = options[skip:], probs[skip:]\n",
    "    preds = []\n",
    "    for i in range(probs.shape[2]):\n",
    "        idx = np.unravel_index(np.argmax(probs[:,:,i]),probs[:,:,i].shape)\n",
    "        preds.append(options[idx[0]][idx[1],i])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:00, 99.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all valid batches\n",
    "valid_batches = []\n",
    "for batch in tqdm(valid_dataset):\n",
    "    valid_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different hyper-parameters\n",
    "bonus = [0, 0.05, 0.1, 0.2, 0.3]\n",
    "alpha = [0.6, 0.7, 0.8, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_options, list_probs =[], []\n",
    "for b in bonus:\n",
    "    batch_ops, batch_probs =[], []\n",
    "    for batch in tqdm(valid_dataset,total = 26):    \n",
    "        ops,probs = get_various_runs(model,batch,word2idx_fr,bonus=b)\n",
    "        batch_ops.append(ops)\n",
    "        batch_probs.append(probs)\n",
    "        \n",
    "    list_options.append(batch_ops)\n",
    "    list_probs.append(batch_probs)\n",
    "\n",
    "options,probs  = list_options, list_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bleu Score\n",
    "for i,b in enumerate(bonus):\n",
    "    for k,alp in enumerate(alpha):\n",
    "        bleu_beam.reset_states()\n",
    "        for j,batch_options in enumerate(options[i]):\n",
    "            batch_probs = probs[i][j]\n",
    "            preds = process_batch(batch_options,batch_probs,alp,skip=10)\n",
    "            bleu_beam.update_state(valid_batches[j]['labels'], preds, idx2word_fr, idx = True)\n",
    "        print(f\"Bleu Score for bonus: {b} , Alpha: {alp} = {bleu_beam.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See some examples\n",
    "steps,width = 150, 5\n",
    "hist, hist_probs = tf_beam_search(model,valid_batches[0],steps,width,word2idx_fr, bonus = 0.4)\n",
    "hist_probs = hist_probs[1:] / (np.arange(1,steps) ** 0.7).reshape(-1,1,1)\n",
    "output = model(valid_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:\n",
      "as we know the reduction of regional disparities is one of the fundamental aims of the eu\n",
      "\n",
      "Target:\n",
      "Comme nous le savons , la disparition des disparités régionales constitue un des objectifs fondamentaux de l' ue .\n"
     ]
    }
   ],
   "source": [
    "idx=11\n",
    "print(\"Source:\")\n",
    "print(utils.generate_sentence(valid_batches[0]['inputs'][idx].numpy(),idx2word_en))\n",
    "print(\"\\nTarget:\")\n",
    "print(utils.generate_sentence(valid_batches[0]['labels'][idx].numpy(),idx2word_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Prediction:\n",
      "Comme nous le savons la réduction des émissions de conum , il est essentiel de réduire les émissions de conum .\n",
      "\n",
      "Beam Search Prediction:\n",
      "Comme nous le savons tous , la réduction des disparités régionales est un des objectifs fondamentaux de l' ue .\n"
     ]
    }
   ],
   "source": [
    "print(\"Greedy Prediction:\")\n",
    "print(utils.generate_sentence_from_probabilities(output[idx],idx2word_fr))\n",
    "\n",
    "skip = 10\n",
    "i = np.unravel_index(np.argmax(hist_probs[skip:,:,idx]),hist_probs[skip:,:,idx].shape)\n",
    "print(\"\\nBeam Search Prediction:\")\n",
    "print(utils.generate_sentence(hist[skip:][i[0]][i[1],idx],idx2word_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
